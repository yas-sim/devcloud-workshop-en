{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to develop a C++ project using OpenVINO and run it on DevCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you'll learn how to develop a image classification C++ application using OpenVINO, and how to run it on the DevCloud.\n",
    "- Creating OpenVINO C++ project and build it (CMake)\n",
    "- Running the built binary on the DevCloud development server\n",
    "- Running the built binary on an edge computing node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 1. Create and build C++ application and run it on the DevCloud development server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Prepareing a DL model\n",
    "Let's start with downloading a DL model for this exercise. In this exercise, we'll use an image classification sample program, so you need to download an appropriate model for that purpose. This time, we'll download a `squeezenet1.1` model.  \n",
    "Use an OpenVINO support tool `Model downloader` to download the model, and convert the downloaded model into OpenVINO IR model using `Model converter`. After you run `Model converter`, you'll see 5 `[ SUCCESS ]` at the end of the log output and the model conversion is successfully finished. The converted IR model files can be found in `./public/squezenet1.1/FP16/` directory.  \n",
    "\n",
    "**Memo:** The `Model optimizer` in OpenVINO is the primary tool to convert a trained deep learning models generated by generic DL frameworks such as TensorFlow, ONNX, and Caffe. The `Model converter` is a front-end tool for the `Model optimizer` and it will simplify model conversion. The `Model converter` is a handy tool but it works only for the DL models downloaded with the `Model downloader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --name squeezenet1.1\n",
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/converter.py  --name squeezenet1.1 --precisions FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Preparing an image file and class label text file for inferencing\n",
    "- Input image file\n",
    " - Copy an image file from OpenVINO demo directory (`car.png`)\n",
    "- Class label text file\n",
    " - The `squeezenet1.1` model is trained with the ImageNet data-set which has 1000 classes\n",
    " - It is almost impossible to understand the result if program just output the class number, we'll use the class label text data (`synset_words.txt`) and output the class name in the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy an image file\n",
    "!cp $INTEL_OPENVINO_DIR/deployment_tools/demo/car.png .\n",
    "# Download a class label text file\n",
    "!curl -O https://raw.githubusercontent.com/HoldenCaulfieldRye/caffe/master/data/ilsvrc12/synset_words.txt\n",
    "\n",
    "from IPython.display import Image\n",
    "Image('car.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Preparing a C++ source code\n",
    "For the sake of simplicity, we'll use `%%writefile` magic command to generate a C++ source code file here. You can use other common and generic way such as using a text editor, or uploading your source code to the server.  \n",
    "The program is using OpenVINO to classify what's in the picture using deep leaning algorithm. We don't explain the source code or OpenVINO API here but the code is only 50+ lines and very simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile main.cpp\n",
    "#include <iostream>\n",
    "#include <fstream>\n",
    "#include <vector>\n",
    "\n",
    "#include <opencv2/opencv.hpp>\n",
    "#include <inference_engine.hpp>\n",
    "\n",
    "namespace ie = InferenceEngine;\n",
    "\n",
    "int main(int argc, char *argv[]) {\n",
    "\n",
    "  std::ifstream label_file(\"synset_words.txt\");  \n",
    "  std::string str;\n",
    "  std::vector<std::string> labels;\n",
    "  while(getline(label_file, str)) labels.push_back(str);\n",
    "  label_file.close();\n",
    "\n",
    "  // Creating an Inference Engine core object\n",
    "  ie::Core ie;\n",
    "\n",
    "  // Loading a DL model to memory\n",
    "  ie::CNNNetwork network = ie.ReadNetwork(\"public/squeezenet1.1/FP16/squeezenet1.1.xml\",\n",
    "                                          \"public/squeezenet1.1/FP16/squeezenet1.1.bin\");\n",
    "  // Setting up the input blob\n",
    "  std::shared_ptr<ie::InputInfo> input_info = \n",
    "                           network.getInputsInfo().begin()->second;\n",
    "  std::string input_name = network.getInputsInfo().begin()->first;\n",
    "  input_info->getPreProcess().setResizeAlgorithm(ie::RESIZE_BILINEAR);\n",
    "  input_info->setLayout(ie::Layout::NHWC);\n",
    "  input_info->setPrecision(ie::Precision::U8);\n",
    "\n",
    "  // Setting up the output blob\n",
    "  ie::DataPtr output_info = network.getOutputsInfo().begin()->second;\n",
    "  std::string output_name = network.getOutputsInfo().begin()->first;\n",
    "  output_info->setPrecision(ie::Precision::FP32);\n",
    "\n",
    "  // Set the DL model to an Inference Engine object\n",
    "  ie::ExecutableNetwork executable_network = ie.LoadNetwork(network, \"CPU\");\n",
    "  ie::InferRequest infer_request = executable_network.CreateInferRequest();\n",
    "\n",
    "// main inferencing loop - start -------\n",
    "\n",
    "  cv::Mat image = cv::imread(\"car.png\");   // Loading an image\n",
    "\n",
    "  ie::TensorDesc tDesc(ie::Precision::U8, \n",
    "      {1, 3, static_cast<long unsigned int>(image.rows), \n",
    "             static_cast<long unsigned int>(image.cols) }, ie::Layout::NHWC);\n",
    "  infer_request.SetBlob(input_name, ie::make_shared_blob<uint8_t>(tDesc, image.data));\n",
    "\n",
    "  infer_request.Infer();      // Do inferencing\n",
    "\n",
    "  // Displaying result\n",
    "  float* output = infer_request.GetBlob(output_name)->buffer();\n",
    "  std::cout << \"\\nresults\\n------------------\" << std::endl;\n",
    "  std::vector<int> idx;\n",
    "  for(int i=0; i<1000; i++) idx.push_back(i);\n",
    "  std::sort(idx.begin(), idx.end(), [output](const int& left, const int& right) { return output[left]>output[right]; } );\n",
    "  for (size_t id = 0; id < 5; ++id)  std::cout << id <<  \" : \" << idx[id] << \" : \" << \n",
    "    std::fixed<< std::setprecision(2) << output[idx[id]]*100 << \"% \" << labels[idx[id]] << std::endl;\n",
    "\n",
    "// main inferencing loop - end ------\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Preparing `CMakeLists.txt` for building the source\n",
    "Create a `cmake` configuration file (`CMakeLists.txt`) using `%%writefile` magic command.\n",
    "You can reuse this configuration file for your own project by just modifying `set(TARGET_NAME` and `add_executable(` part appropriately. You may need to add libraries to `target_link_libraries()` or add `find_package()` if you use extra libraries in your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile CMakeLists.txt\n",
    "# Sample CMakeLists.txt file for an OpenVINO Inference Engine project\n",
    "cmake_minimum_required (VERSION 2.8.1)\n",
    "\n",
    "set(TARGET_NAME simple_cnn)     # name of executable file\n",
    "set(CMAKE_BUILD_TYPE \"Release\")\n",
    "\n",
    "set(CMAKE_CXX_STANDARD 11)\n",
    "set(CMAKE_CXX_STANDARD_REQUIRED ON)\n",
    "set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -fPIE\")\n",
    "\n",
    "find_package(InferenceEngine 1.1 REQUIRED)\n",
    "find_package(OpenCV REQUIRED)\n",
    "add_definitions(-DUSE_OPENCV)\n",
    "\n",
    "include_directories( ${InferenceEngine_INCLUDE_DIRS} ${OpenCV_INCLUDE_DIRS} )\n",
    "link_directories( )\n",
    "\n",
    "add_executable( ${TARGET_NAME} main.cpp )    # list of source file(s)\n",
    "set_target_properties(${TARGET_NAME} PROPERTIES \"CMAKE_CXX_FLAGS\" \"${CMAKE_CXX_FLAGS}\")\n",
    "target_link_libraries(${TARGET_NAME} ${InferenceEngine_LIBRARIES} ${OpenCV_LIBS} )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Building the application\n",
    "Create a `build` directory and build the C++ source code.  \n",
    "The `!` command will invoke a child-shell, run the command and destroy the child-shell every time you run a command. So the change you made for the shell will be discarded and won't take effect.  \n",
    "For example, the `cd` command will change the working directory but it is effective until the end of the line.  \n",
    "So, we need to put everything in a line using `&&`.\n",
    "\n",
    "The built binary can be found as `./build/simple_cnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p build && cd build && cmake .. && make && ls -l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Running the built binary on the DevCloud development server\n",
    "Run the build binary **on the DevCloud development server**.  \n",
    "This program doesn't take any option parameters. The input image file name, model file name, and class label file name are hard coded in the source code.  \n",
    "\n",
    "Run the binary and check if the output is correct and expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!build/simple_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## 2. Submitting a job and run a C++ program on a DevCloud edge computing node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we run the C++ program on the development server.  \n",
    "Here, we'll submit a job and run the same program on an edge computing node on the DevCloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Creating a job script file\n",
    "The working directory of your job on an edge computing node is the user home directory (`~`, , `$HOME`, `/home/<userid>`), regardless of your current working directory on the host server. \n",
    "Please change the working directory appropriately and make sure that your job is working on the desired directory on the edge computing node.  \n",
    "**The same storage is visible from the edge computing node**. You don't need to transfer related files such as input image file to the edge computing node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile job.sh\n",
    "cd ~/devcloud-workshop-en\n",
    "build/simple_cnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Submitting and running a job on an edge computing node\n",
    "We don't specify the specific edge computing node here, so, the job will be run on an edge computing node adhocery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit a job\n",
    "job_id=!qsub job.sh\n",
    "\n",
    "# generate log file name from job_id\n",
    "job_num = job_id[0].split('.')[0]\n",
    "log_file='job.sh.o'+job_num\n",
    "err_file='job.sh.e'+job_num\n",
    "print('job_id={}, log_file={}'.format(job_id, log_file))\n",
    "\n",
    "import time\n",
    "def waitForJobCompletion(jobNumber):\n",
    "    print('Waiting for job completion...', end='')\n",
    "    running=True\n",
    "    while running:\n",
    "        time.sleep(1)\n",
    "        running=False\n",
    "        status_list=!qstat         # Check job status\n",
    "        for status in status_list:\n",
    "            if jobNumber in status:    # if job_num is found in the status list, the job is still running\n",
    "                running = True\n",
    "        print(status.split()[4], end='')\n",
    "    print('...Job {} completed'.format(job_num))    \n",
    "\n",
    "# wait for the job to complete\n",
    "waitForJobCompletion(job_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Checking the output log\n",
    "Display the output log file (`job.sh.o<job#>`) and confirm whether the same result is output as when we run it on the development server or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The log_file is set in the previous cell\n",
    "import os\n",
    "os.environ['log_file']=log_file\n",
    "\n",
    "!cat $log_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Now, you have learnt how to create, build and run a C++ project on DevCloud.\n",
    "Let's modify the sample program here or, try your own code on DevCloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of the course"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## < Appendix > [Automate evaluation work on DevCloud](./automated-testing.ipynb>automated-testing.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
